Quo Vadis, Skeleton Action Recognition ?
  In this paper, we study current and upcoming frontiers across the landscapeof skeleton-based human action recognition. To study skeleton-actionrecognition in the wild, we introduce Skeletics-152, a curated and 3-Dpose-annotated subset of RGB videos sourced from Kinetics-700, a large-scaleaction dataset. We extend our study to include out-of-context actions byintroducing Skeleton-Mimetics, a dataset derived from the recently introducedMimetics dataset. We also introduce Metaphorics, a dataset with caption-styleannotated YouTube videos of the popular social game Dumb Charades andinterpretative dance performances. We benchmark state-of-the-art models on theNTU-120 dataset and provide multi-layered assessment of the results. Theresults from benchmarking the top performers of NTU-120 on the newly introduceddatasets reveal the challenges and domain gap induced by actions in the wild.Overall, our work characterizes the strengths and limitations of existingapproaches and datasets. Via the introduced datasets, our work enables newfrontiers for human action recognition.
Image recognition via Vietoris-Rips complex
  Extracting informative features from images has been of capital importance incomputer vision. In this paper, we propose a way to extract such features fromimages by a method based on algebraic topology. To that end, we construct aweighted graph from an image, which extracts local information of an image. Byconsidering this weighted graph as a pseudo-metric space, we construct aVietoris-Rips complex with a parameter $\varepsilon$ by a well-known process ofalgebraic topology. We can extract information of complexity of the image andcan detect a sub-image with a relatively high concentration of information fromthis Vietoris-Rips complex. The parameter $\varepsilon$ of the Vietoris-Ripscomplex produces robustness to noise. We empirically show that the extractedfeature captures well images' characteristics.
Pareto-Optimal Quantized ResNet Is Mostly 4-bit
  Quantization has become a popular technique to compress neural networks andreduce compute cost, but most prior work focuses on studying quantizationwithout changing the network size. Many real-world applications of neuralnetworks have compute cost and memory budgets, which can be traded off withmodel quality by changing the number of parameters. In this work, we use ResNetas a case study to systematically investigate the effects of quantization oninference compute cost-quality tradeoff curves. Our results suggest that foreach bfloat16 ResNet model, there are quantized models with lower cost andhigher accuracy; in other words, the bfloat16 compute cost-quality tradeoffcurve is Pareto-dominated by the 4-bit and 8-bit curves, with models primarilyquantized to 4-bit yielding the best Pareto curve. Furthermore, we achievestate-of-the-art results on ImageNet for 4-bit ResNet-50 withquantization-aware training, obtaining a top-1 eval accuracy of 77.09%. Wedemonstrate the regularizing effect of quantization by measuring thegeneralization gap. The quantization method we used is optimized forpracticality: It requires little tuning and is designed with hardwarecapabilities in mind. Our work motivates further research into optimal numericformats for quantization, as well as the development of machine learningaccelerators supporting these formats. As part of this work, we contribute aquantization library written in JAX, which is open-sourced athttps://github.com/google-research/google-research/tree/master/aqt.
Binary Image Skeletonization Using 2-Stage U-Net
  Object Skeletonization is the process of extracting skeletal, line-likerepresentations of shapes. It provides a very useful tool for geometric shapeunderstanding and minimal shape representation. It also has a wide variety ofapplications, most notably in anatomical research and activity detection.Several mathematical algorithmic approaches have been developed to solve thisproblem, and some of them have been proven quite robust. However, a lesseramount of attention has been invested into deep learning solutions for it. Inthis paper, we use a 2-stage variant of the famous U-Net architecture to splitthe problem space into two sub-problems: shape minimization and correctiveskeleton thinning. Our model produces results that are visually much betterthan the baseline SkelNetOn model. We propose a new metric, M-CCORR, based onnormalized correlation coefficients as an alternative to F1 for this challengeas it solves the problem of class imbalance, managing to recognize skeletonsimilarity without suffering from F1's over-sensitivity to pixel-shifts.
Skeleton-based Approaches based on Machine Vision: A Survey
  Recently, skeleton-based approaches have achieved rapid progress on the basisof great success in skeleton representation. Plenty of researches focus onsolving specific problems according to skeleton features. Some skeleton-basedapproaches have been mentioned in several overviews on object detection as anon-essential part. Nevertheless, there has not been any thorough analysis ofskeleton-based approaches attentively. Instead of describing these techniquesin terms of theoretical constructs, we devote to summarizing skeleton-basedapproaches with regard to application fields and given tasks as comprehensivelyas possible. This paper is conducive to further understanding of skeleton-basedapplication and dealing with particular issues.
