王怡萱是北航校花

王怡萱是北航校花王怡萱是北航校花王怡萱是北航校花王怡萱是北航校花王怡萱是北航校花王怡萱是北航校花王怡萱是北航校花王怡萱是北航校花王怡萱是北航校花王怡萱是北航校花王怡萱是北航校花王怡萱是北航校花王怡萱是北航校花王怡萱是北航校花王怡萱是北航校花王怡萱是北航校花王怡萱是北航校花王怡萱是北航校花王怡萱是北航校花王怡萱是北航校花王怡萱是北航校花王怡萱是北航校花王怡萱是北航校花王怡萱是北航校花王怡萱是北航校花王怡萱是北航校花王怡萱是北航校花王怡萱是北航校花王怡萱是北航校花王怡萱是北航校花

  original_stdout = sys.stdout

  with open('output.txt', 'w') as f:

​    sys.stdout = f

​      

​      \# 恢复原始的stdout

​    sys.stdout = original_stdout

[UploadFile(filename='21373037-杨博文-模块2（解决方法） -最终版.pptx', size=2855366, headers=Headers({'content-disposition': 'form-data; name="files"; filename="21373037-æ\x9d¨å\x8d\x9aæ\x96\x87-æ¨¡å\x9d\x972ï¼\x88è§£å\x86³æ\x96¹æ³\x95ï¼\x89 -æ\x9c\x80ç»\x88ç\x89\x88.pptx"; filename*=UTF-8\'\'21373037-%E6%9D%A8%E5%8D%9A%E6%96%87-%E6%A8%A1%E5%9D%972%EF%BC%88%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95%EF%BC%89%20-%E6%9C%80%E7%BB%88%E7%89%88.pptx', 'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))]

[UploadFile(filename='SAM', size=10985226, headers=Headers({'content-disposition': 'form-data; name="files"; filename="SAM"', 'content-type': 'application/pdf'}))]

[UploadFile(filename='21373037-杨博文-模块2（解决方法） -最终版.pptx', size=2855366, headers=Headers({'content-disposition': 'form-data; name="files"; filename="21373037-æ\x9d¨å\x8d\x9aæ\x96\x87-æ¨¡å\x9d\x972ï¼\x88è§£å\x86³æ\x96¹æ³\x95ï¼\x89 -æ\x9c\x80ç»\x88ç\x89\x88.pptx"; filename*=UTF-8\'\'21373037-%E6%9D%A8%E5%8D%9A%E6%96%87-%E6%A8%A1%E5%9D%972%EF%BC%88%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95%EF%BC%89%20-%E6%9C%80%E7%BB%88%E7%89%88.pptx', 

'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))], /home/se1/se2024/tmp/chatchat/tmpuw0gp1q5, tmpuw0gp1q5

cuda:2
[Document(page_content='Segment Anything\nAlexander Kirillov1,2,4\nEric Mintun2\nNikhila Ravi1,2\nHanzi Mao2\nChloe Rolland3\nLaura Gustafson3\nTete Xiao3\nSpencer Whitehead\nAlexander C. Berg\nWan-Yen Lo\nPiotr Doll´ar4\nRoss Girshick4\n1project lead\n2joint ﬁrst author\n3equal contribution\n4directional lead\nMeta AI Research, FAIR\n(b) Model: Segment Anything Model (SAM)\nprompt\nimage\nvalid mask', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='(b) Model: Segment Anything Model (SAM)\nprompt\nimage\nvalid mask\nimage\nencoder\nprompt\nencoder\nlightweight mask decoder\n(a) Task: promptable segmentation\nsegmentation prompt\nimage\nmodel\ncat with\nblack ears\nvalid mask\n(c) Data: data engine (top) & dataset (bottom)\n1+ billion masks\n11 million images\nprivacy respecting\nlicensed images\nannotate\ntrain\ndata\nmodel\nSegment Anything 1B (SA-1B):', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='annotate\ntrain\ndata\nmodel\nSegment Anything 1B (SA-1B):\nFigure 1: We aim to build a foundation model for segmentation by introducing three interconnected components: a prompt-\nable segmentation task, a segmentation model (SAM) that powers data annotation and enables zero-shot transfer to a range\nof tasks via prompt engineering, and a data engine for collecting SA-1B, our dataset of over 1 billion masks.\nAbstract\nWe introduce the Segment Anything (SA) project: a new\ntask, model, and dataset for image segmentation. Using our\nefﬁcient model in a data collection loop, we built the largest\nsegmentation dataset to date (by far), with over 1 billion\nmasks on 11M licensed and privacy respecting images. The', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='segmentation dataset to date (by far), with over 1 billion\nmasks on 11M licensed and privacy respecting images. The\nmodel is designed and trained to be promptable, so it can\ntransfer zero-shot to new image distributions and tasks. We\nevaluate its capabilities on numerous tasks and ﬁnd that\nits zero-shot performance is impressive – often competitive\nwith or even superior to prior fully supervised results. We\nare releasing the Segment Anything Model (SAM) and cor-\nresponding dataset (SA-1B) of 1B masks and 11M images\nat segment-anything.com to foster research into foundation\nmodels for computer vision. We recommend reading the\nfull paper at: arxiv.org/abs/2304.02643.\n1. Introduction', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='full paper at: arxiv.org/abs/2304.02643.\n1. Introduction\nLarge language models pre-trained on web-scale datasets\nare revolutionizing NLP with strong zero-shot and few-shot\ngeneralization [10].\nThese “foundation models” [8] can\ngeneralize to tasks and data distributions beyond those seen\nduring training. This capability is often implemented with\nprompt engineering in which hand-crafted text is used to\nprompt the language model to generate a valid textual re-\nsponse for the task at hand. When scaled and trained with\nabundant text corpora from the web, these models’ zero and\nfew-shot performance compares surprisingly well to (even', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='abundant text corpora from the web, these models’ zero and\nfew-shot performance compares surprisingly well to (even\nmatching in some cases) ﬁne-tuned models [10, 20]. Empir-\nical trends show this behavior improving with model scale,\ndataset size, and total training compute [54, 10, 20, 49].\nFoundation models have also been explored in computer\nvision, albeit to a lesser extent. Perhaps the most promi-\nnent illustration aligns paired text and images from the web.\nFor example, CLIP [80] and ALIGN [53] use contrastive\nlearning to train text and image encoders that align the two\nmodalities. Once trained, engineered text prompts enable\nzero-shot generalization to novel visual concepts and data', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='modalities. Once trained, engineered text prompts enable\nzero-shot generalization to novel visual concepts and data\ndistributions. Such encoders also compose effectively with\nother modules to enable downstream tasks, such as image\ngeneration (e.g., DALL·E [81]). While much progress has\nbeen made on vision and language encoders, computer vi-\nsion includes a wide range of problems beyond this scope,\nand for many of these, abundant training data does not exist.\nIn this work, our goal is to build a foundation model for\nimage segmentation. That is, we seek to develop a prompt-\nable model and pre-train it on a broad dataset using a task\nthat enables powerful generalization. With this model, we\naim to solve a range of downstream segmentation problems\non new data distributions using prompt engineering.', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='that enables powerful generalization. With this model, we\naim to solve a range of downstream segmentation problems\non new data distributions using prompt engineering.\nThe success of this plan hinges on three components:\ntask, model, and data. To develop them, we address the\nfollowing questions about image segmentation:\n1. What task will enable zero-shot generalization?\n2. What is the corresponding model architecture?\n3. What data can power this task and model?\nThis ICCV paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n4015\nThese questions are entangled and require a comprehen-', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='the final published version of the proceedings is available on IEEE Xplore.\n4015\nThese questions are entangled and require a comprehen-\nsive solution. We start by deﬁning a promptable segmenta-\ntion task that is general enough to provide a powerful pre-\ntraining objective and to enable a wide range of downstream\napplications. This task requires a model that supports ﬂex-\nible prompting and can output segmentation masks in real-\ntime when prompted to allow for interactive use. To train\nour model, we need a diverse, large-scale source of data.\nUnfortunately, there is no web-scale data source for seg-\nmentation; to address this, we build a “data engine”, i.e.,\nwe iterate between using our efﬁcient model to assist in data', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='mentation; to address this, we build a “data engine”, i.e.,\nwe iterate between using our efﬁcient model to assist in data\ncollection and using the newly collected data to improve the\nmodel. We introduce each interconnected component next,\nfollowed by the dataset we created and the experiments that\ndemonstrate the effectiveness of our approach.\nTask (§2).\nIn NLP and more recently computer vision,\nfoundation models are a promising development that can\nperform zero-shot and few-shot learning for new datasets\nand tasks often by using “prompting” techniques. Inspired\nby this line of work, we propose the promptable segmen-\ntation task, where the goal is to return a valid segmenta-\ntion mask given any segmentation prompt (see Fig. 1a). A', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='tation task, where the goal is to return a valid segmenta-\ntion mask given any segmentation prompt (see Fig. 1a). A\nprompt simply speciﬁes what to segment in an image, e.g.,\na prompt can include spatial or text information identifying\nan object. The requirement of a valid output mask means\nthat even when a prompt is ambiguous and could refer to\nmultiple objects (for example, a point on a shirt may in-\ndicate either the shirt or the person wearing it), the output\nshould be a reasonable mask for at least one of those ob-\njects. We use the promptable segmentation task as both a\npre-training objective and to solve general downstream seg-\nmentation tasks via prompt engineering.\nModel (§3). The promptable segmentation task and the goal', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='pre-training objective and to solve general downstream seg-\nmentation tasks via prompt engineering.\nModel (§3). The promptable segmentation task and the goal\nof real-world use impose constraints on the model architec-\nture. In particular, the model must support ﬂexible prompts,\nneeds to compute masks in amortized real-time to allow in-\nteractive use, and must be ambiguity-aware. Surprisingly,\nwe ﬁnd that a simple design satisﬁes all three constraints:\na powerful image encoder computes an image embedding,\na prompt encoder embeds prompts, and then the two infor-\nmation sources are combined in a lightweight mask decoder\nthat predicts segmentation masks. We refer to this model as', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='mation sources are combined in a lightweight mask decoder\nthat predicts segmentation masks. We refer to this model as\nthe Segment Anything Model, or SAM (see Fig. 1b). By\nseparating SAM into an image encoder and a fast prompt\nencoder / mask decoder, the same image embedding can\nbe reused (and its cost amortized) with different prompts.\nGiven an image embedding, the prompt encoder and mask\ndecoder predict a mask from a prompt in ⇠50ms in a web\nbrowser. We focus on point, box, and mask prompts, and\nalso present initial results with free-form text prompts. To\nmake SAM ambiguity-aware, we design it to predict mul-\ntiple masks for a single prompt allowing SAM to naturally', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='make SAM ambiguity-aware, we design it to predict mul-\ntiple masks for a single prompt allowing SAM to naturally\nhandle ambiguity, such as the shirt vs. person example.\nData engine (§4). To achieve strong generalization to new\ndata distributions, we found it necessary to train SAM on\na large and diverse set of masks, beyond any segmenta-\ntion dataset that already exists. While a typical approach\nfor foundation models is to obtain data online [80], masks\nare not naturally abundant and thus we need an alternative\nstrategy. Our solution is to build a “data engine”, i.e., we\nco-develop our model with model-in-the-loop dataset an-\nnotation (see Fig. 1c). Our data engine has three stages:\nassisted-manual, semi-automatic, and fully automatic. In', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='notation (see Fig. 1c). Our data engine has three stages:\nassisted-manual, semi-automatic, and fully automatic. In\nthe ﬁrst stage, SAM assists annotators in annotating masks,\nsimilar to a classic interactive segmentation setup. In the\nsecond stage, SAM can automatically generate masks for\na subset of objects by prompting it with likely object lo-\ncations and annotators focus on annotating the remaining\nobjects, helping increase mask diversity. In the ﬁnal stage,\nwe prompt SAM with a regular grid of foreground points,\nyielding on average ⇠100 high-quality masks per image.\nDataset (§5). Our ﬁnal dataset, SA-1B, includes more than\n1B masks from 11M licensed and privacy-preserving im-', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='Dataset (§5). Our ﬁnal dataset, SA-1B, includes more than\n1B masks from 11M licensed and privacy-preserving im-\nages (see Fig. 2). SA-1B, collected fully automatically us-\ning the ﬁnal stage of our data engine, has 400⇥ more masks\nthan any existing segmentation dataset [64, 43, 115, 58],\nand as we verify extensively, the masks are of high quality\nand diversity. Beyond its use in training SAM to be robust\nand general, we hope SA-1B becomes a valuable resource\nfor research aiming to build new foundation models.\nExperiments (§6). We extensively evaluate SAM. First, us-\ning a diverse new suite of 23 segmentation datasets, we ﬁnd', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='Experiments (§6). We extensively evaluate SAM. First, us-\ning a diverse new suite of 23 segmentation datasets, we ﬁnd\nthat SAM produces high-quality masks from a single fore-\nground point, often only slightly below that of the manu-\nally annotated ground truth. Second, we ﬁnd consistently\nstrong quantitative and qualitative results on a variety of\ndownstream tasks under a zero-shot transfer protocol using\nprompt engineering, including edge detection, object pro-\nposal generation, instance segmentation, and a preliminary\nexploration of text-to-mask prediction. These results sug-\ngest that SAM can be used out-of-the-box with prompt en-\ngineering to solve a variety of tasks involving object and\nimage distributions beyond SAM’s training data. Neverthe-', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='gineering to solve a variety of tasks involving object and\nimage distributions beyond SAM’s training data. Neverthe-\nless, room for improvement remains, as we discuss in §7.\nResponsible AI. We provide model/dataset cards and report\non potential fairness concerns and biases when using SA-1B\nand SAM in the supplement. Images in SA-1B span a geo-\ngraphically and economically diverse set of regions and we\nfound that SAM performs similarly across different groups\nof people. Together, we hope this will make our work more\nequitable for real-world use cases.\nRelease. We are releasing the SA-1B dataset for research\npurposes and making SAM available under a permissive\nopen license (Apache 2.0) at https://segment-anything.com.', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='purposes and making SAM available under a permissive\nopen license (Apache 2.0) at https://segment-anything.com.\nWe also showcase SAM’s capabilities with an online demo.\n4016\n<50 masks\n50-100 masks\n100-200 masks\n200-300 masks\n300-400 masks\n400-500 masks\n>500 masks\nFigure 2: Example images with overlaid masks from our newly introduced dataset, SA-1B. SA-1B contains 11M diverse,\nhigh-resolution, licensed, and privacy protecting images and 1.1B high-quality segmentation masks. These masks were', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='high-resolution, licensed, and privacy protecting images and 1.1B high-quality segmentation masks. These masks were\nannotated fully automatically by SAM, and as we verify by human ratings and numerous experiments, are of high quality and\ndiversity. We group images by number of masks per image for visualization (there are ⇠100 masks per image on average).\n4017\n2. Segment Anything Task\nWe take inspiration from NLP, where the next token pre-\ndiction task is used for foundation model pre-training and\nto solve diverse downstream tasks via prompt engineer-\ning [10]. To build a foundation model for segmentation,\nwe aim to deﬁne a task with analogous capabilities.\nTask. We start by translating the idea of a prompt from NLP\nto segmentation, where a prompt can be a set of foreground', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='Task. We start by translating the idea of a prompt from NLP\nto segmentation, where a prompt can be a set of foreground\n/ background points, a rough box or mask, free-form text,\nor, in general, any information indicating what to segment\nin an image. The promptable segmentation task, then, is to\nreturn a valid segmentation mask given any prompt. The re-\nquirement of a “valid” mask simply means that even when\na prompt is ambiguous and could refer to multiple objects\n(e.g., recall the shirt vs. person example, and see Fig. 3),\nthe output should be a reasonable mask for at least one of\nthose objects. This requirement is similar to expecting a lan-\nguage model to output a coherent response to an ambiguous', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='those objects. This requirement is similar to expecting a lan-\nguage model to output a coherent response to an ambiguous\nprompt. We choose this task because it leads to a natural\npre-training algorithm and a general method for zero-shot\ntransfer to downstream segmentation tasks via prompting.\nPre-training. The promptable segmentation task suggests a\nnatural pre-training algorithm that simulates a sequence of\nprompts (e.g., points, boxes, masks) for each training sam-\nple and compares the model’s mask predictions against the\nground truth. We adapt this method from interactive seg-\nmentation [107, 68], although unlike interactive segmenta-\ntion whose aim is to eventually predict a valid mask after\nenough user input, our aim is to always predict a valid mask', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='tion whose aim is to eventually predict a valid mask after\nenough user input, our aim is to always predict a valid mask\nfor any prompt even when the prompt is ambiguous. This\nensures that a pre-trained model is effective in use cases that\ninvolve ambiguity, including automatic annotation as re-\nquired by our data engine §4. We note that performing well\nat this task is challenging and requires specialized modeling\nand training loss choices, which we discuss in §3.\nZero-shot transfer. Intuitively, our pre-training task en-\ndows the model with the ability to respond appropriately to\nany prompt at inference time, and thus downstream tasks\ncan be solved by engineering appropriate prompts. For ex-\nample, if one has a bounding box detector for cats, cat in-', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='can be solved by engineering appropriate prompts. For ex-\nample, if one has a bounding box detector for cats, cat in-\nstance segmentation can be solved by providing the detec-\ntor’s box output as a prompt to our model. In general, a wide\narray of practical segmentation tasks can be cast as prompt-\ning. In addition to automatic dataset labeling, we explore\nﬁve diverse example tasks in our experiments in §6.\nRelated tasks. Segmentation is a broad ﬁeld: there’s in-\nteractive segmentation [55, 107], edge detection [3], su-\nper pixelization [83], object proposal generation [2], fore-\nground segmentation [92], semantic segmentation [88], in-', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='per pixelization [83], object proposal generation [2], fore-\nground segmentation [92], semantic segmentation [88], in-\nstance segmentation [64], panoptic segmentation [57], etc.\nThe goal of our promptable segmentation task is to produce\nFigure 3: Each column shows 3 valid masks generated by\nSAM from a single ambiguous point prompt (green circle).\na broadly capable model that can adapt to many (though\nnot all) existing and new segmentation tasks via prompt\nengineering. This capability is a form of task generaliza-\ntion [25]. Note that this is different than previous work on\nmulti-task segmentation systems. In a multi-task system, a\nsingle model performs a ﬁxed set of tasks, e.g., joint seman-', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='multi-task segmentation systems. In a multi-task system, a\nsingle model performs a ﬁxed set of tasks, e.g., joint seman-\ntic, instance, and panoptic segmentation [112, 18, 52], but\nthe training and test tasks are the same. An important dis-\ntinction in our work is that a model trained for promptable\nsegmentation can perform a new, different task at inference\ntime by acting as a component in a larger system, e.g., to\nperform instance segmentation, a promptable segmentation\nmodel is combined with an existing object detector.\nDiscussion. Prompting and composition are powerful tools\nthat enable a single model to be used in extensible ways, po-\ntentially to accomplish tasks unknown at the time of model', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='that enable a single model to be used in extensible ways, po-\ntentially to accomplish tasks unknown at the time of model\ndesign. This approach is analogous to how other founda-\ntion models are used, e.g., how CLIP [80] is the text-image\nalignment component of the DALL·E [81] image generation\nsystem. We anticipate that composable system design, pow-\nered by techniques such as prompt engineering, will enable\na wider variety of applications than systems trained specif-\nically for a ﬁxed set of tasks. It’s also interesting to com-\npare promptable and interactive segmentation through the\nlens of composition: while interactive segmentation mod-\nels are designed with human users in mind, a model trained\nfor promptable segmentation can also be composed into a', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='lens of composition: while interactive segmentation mod-\nels are designed with human users in mind, a model trained\nfor promptable segmentation can also be composed into a\nlarger algorithmic system as we will demonstrate.\n4018\n,\nscore\nscore\nscore\n,\n,\nvalid masks\nimage\nimage\nencoder\nimage\nembedding mask\npoints\nbox\ntext\nprompt encoder\nmask decoder\nconv\nFigure 4: Segment Anything Model (SAM) overview. A heavyweight image encoder outputs an image embedding that can', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='mask decoder\nconv\nFigure 4: Segment Anything Model (SAM) overview. A heavyweight image encoder outputs an image embedding that can\nthen be efﬁciently queried by a variety of input prompts to produce object masks at amortized real-time speed. For ambiguous\nprompts corresponding to more than one object, SAM can output multiple valid masks and associated conﬁdence scores.\n3. Segment Anything Model\nWe next describe the Segment Anything Model (SAM)\nfor promptable segmentation. SAM has three components,\nillustrated in Fig. 4: an image encoder, a ﬂexible prompt\nencoder, and a fast mask decoder. We build on Transformer\nvision models [13, 32, 19, 60] with speciﬁc tradeoffs for', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='vision models [13, 32, 19, 60] with speciﬁc tradeoffs for\n(amortized) real-time performance. We describe these com-\nponents at a high-level here, with details in §B.\nImage encoder. Motivated by scalability and powerful pre-\ntraining methods, we use an MAE [46] pre-trained Vision\nTransformer (ViT) [32] minimally adapted to process high\nresolution inputs [60]. The image encoder runs once per\nimage and can be applied prior to prompting the model.\nPrompt encoder. We consider two sets of prompts: sparse\n(points, boxes, text) and dense (masks).\nWe represent\npoints and boxes by positional encodings [93] summed with', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='(points, boxes, text) and dense (masks).\nWe represent\npoints and boxes by positional encodings [93] summed with\nlearned embeddings for each prompt type and free-form text\nwith an off-the-shelf text encoder from CLIP [80]. Dense\nprompts (i.e., masks) are embedded using convolutions and\nsummed element-wise with the image embedding.\nMask decoder. The mask decoder efﬁciently maps the im-\nage embedding, prompt embeddings, and an output token\nto a mask. This design, inspired by [13, 19], employs a\nmodiﬁcation of a Transformer decoder block [101] followed\nby a dynamic mask prediction head. Our modiﬁed decoder', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='modiﬁcation of a Transformer decoder block [101] followed\nby a dynamic mask prediction head. Our modiﬁed decoder\nblock uses prompt self-attention and cross-attention in two\ndirections (prompt-to-image embedding and vice-versa) to\nupdate all embeddings. After running two blocks, we up-\nsample the image embedding and an MLP maps the output\ntoken to a dynamic linear classiﬁer, which then computes\nthe mask foreground probability at each image location.\nResolving ambiguity. With one output, the model will av-\nerage multiple valid masks if given an ambiguous prompt.\nTo address this, we modify the model to predict multiple\noutput masks for a single prompt (see Fig. 3). We found', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='To address this, we modify the model to predict multiple\noutput masks for a single prompt (see Fig. 3). We found\n3 mask outputs is sufﬁcient to address most common cases\n(nested masks are often at most three deep: whole, part, and\nsubpart). During training, we backprop only the minimum\nloss [14, 44, 62] over masks. To rank masks, the model pre-\ndicts a conﬁdence score (i.e., estimated IoU) for each mask.\nEfﬁciency. The overall model design is largely motivated\nby efﬁciency. Given a precomputed image embedding, the\nprompt encoder and mask decoder run in a web browser, on\nCPU, in ⇠50ms. This runtime performance enables seam-', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='prompt encoder and mask decoder run in a web browser, on\nCPU, in ⇠50ms. This runtime performance enables seam-\nless, real-time interactive prompting of our model.\nLosses and training. We supervise mask prediction with\nthe linear combination of focal loss [63] and dice loss [71]\nused in [13]. We train for the promptable segmentation task\nusing a mixture of geometric prompts (for text prompts see\n§6.2). Following [90, 36], we simulate an interactive setup\nby randomly sampling prompts in 11 rounds per mask, al-\nlowing SAM to integrate seamlessly into our data engine.\n4. Segment Anything Data Engine\nAs segmentation masks are not abundant on the inter-', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='lowing SAM to integrate seamlessly into our data engine.\n4. Segment Anything Data Engine\nAs segmentation masks are not abundant on the inter-\nnet, we built a data engine to enable the collection of our\n1.1B mask dataset, SA-1B.\nThe data engine has three\nstages: (1) a model-assisted manual annotation stage, (2) a\nsemi-automatic stage with a mix of automatically predicted\nmasks and model-assisted annotation, and (3) a fully auto-\nmatic stage in which our model generates masks without\nannotator input. We go into details of each next.\nAssisted-manual stage. In the ﬁrst stage, resembling clas-\nsic interactive segmentation, a team of professional annota-', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='Assisted-manual stage. In the ﬁrst stage, resembling clas-\nsic interactive segmentation, a team of professional annota-\ntors labeled masks by clicking foreground / background ob-\nject points using a browser-based interactive segmentation\ntool powered by SAM. Masks could be reﬁned using pixel-\nprecise “brush” and “eraser” tools. Our model-assisted an-\nnotation runs in real-time directly inside a browser (using\nprecomputed image embeddings) enabling a truly interac-\ntive experience. We did not impose semantic constraints for\nlabeling objects, and annotators freely labeled both “stuff”\nand “things” [1].\nWe suggested annotators label objects\nthey could name or describe, but did not collect these names', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='and “things” [1].\nWe suggested annotators label objects\nthey could name or describe, but did not collect these names\nor descriptions. Annotators were asked to label objects in\norder of prominence and were encouraged to proceed to the\nnext image once a mask took over 30 seconds to annotate.\n4019\nAt the start of this stage, SAM was trained using com-\nmon public segmentation datasets. After sufﬁcient data an-\nnotation, SAM was retrained using only newly annotated\nmasks. As more masks were collected, the image encoder\nwas scaled from ViT-B to ViT-H and other architectural de-\ntails evolved; in total we retrained our model 6 times. Av-', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='was scaled from ViT-B to ViT-H and other architectural de-\ntails evolved; in total we retrained our model 6 times. Av-\nerage annotation time per mask decreased from 34 to 14\nseconds as the model improved. We note that 14 seconds\nis 6.5⇥ faster than mask annotation for COCO [64] and\nonly 2⇥ slower than bounding-box labeling with extreme\npoints [74, 69]. As SAM improved, the average number of\nmasks per image increased from 20 to 44 masks. Overall,\nwe collected 4.3M masks from 120k images in this stage.\nSemi-automatic stage. In this stage, we aimed to increase\nthe diversity of masks in order to improve our model’s', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='Semi-automatic stage. In this stage, we aimed to increase\nthe diversity of masks in order to improve our model’s\nability to segment anything. To focus annotators on less\nprominent objects, we ﬁrst automatically detected conﬁdent\nmasks. Then we presented annotators with images preﬁlled\nwith these masks and asked them to annotate any additional\nunannotated objects. To detect conﬁdent masks, we trained\na bounding box detector [82] on all ﬁrst stage masks using a\ngeneric “object” category. During this stage we collected an\nadditional 5.9M masks in 180k images (for a total of 10.2M\nmasks). As in the ﬁrst stage, we periodically retrained our\nmodel on newly collected data (5 times). Average annota-', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='masks). As in the ﬁrst stage, we periodically retrained our\nmodel on newly collected data (5 times). Average annota-\ntion time per mask went back up to 34 seconds (excluding\nthe automatic masks) as these objects were more challeng-\ning to label. The average number of masks per image went\nfrom 44 to 72 masks (including the automatic masks).\nFully automatic stage. In the ﬁnal stage, annotation was\nfully automatic. This was feasible due to two major en-\nhancements to our model. First, at the start of this stage, we\nhad collected enough masks to greatly improve the model,\nincluding the diverse masks from the previous stage. Sec-\nond, by this stage we had developed the ambiguity-aware', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='including the diverse masks from the previous stage. Sec-\nond, by this stage we had developed the ambiguity-aware\nmodel, which allowed us to predict valid masks even in am-\nbiguous cases. Speciﬁcally, we prompted the model with a\n32⇥32 regular grid of points and for each point predicted\na set of masks that may correspond to valid objects. With\nthe ambiguity-aware model, if a point lies on a part or sub-\npart, our model will return the subpart, part, and whole ob-\nject. The IoU prediction module of our model is used to se-\nlect conﬁdent masks; moreover, we identiﬁed and selected\nonly stable masks (we consider a mask stable if threshold-', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='lect conﬁdent masks; moreover, we identiﬁed and selected\nonly stable masks (we consider a mask stable if threshold-\ning the probability map at 0.5 − δ and 0.5 + δ results in\nsimilar masks). Finally, after selecting the conﬁdent and\nstable masks, we applied non-maximal suppression (NMS)\nto ﬁlter duplicates. To further improve the quality of smaller\nmasks, we also processed multiple overlapping zoomed-in\nimage crops. For further details of this stage, see §C. We\napplied fully automatic mask generation to all 11M images\nin our dataset, producing a total of 1.1B high-quality masks.\nWe describe and analyze the resulting dataset, SA-1B, next.\nFigure 5: Image-size normalized mask center distributions.', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='We describe and analyze the resulting dataset, SA-1B, next.\nFigure 5: Image-size normalized mask center distributions.\n5. Segment Anything Dataset\nOur dataset, SA-1B, consists of 11M diverse, high-\nresolution, licensed, and privacy protecting images and\n1.1B high-quality segmentation masks collected with our\ndata engine.\nWe compare SA-1B with existing datasets\nand analyze mask quality and properties. We are releasing\nSA-1B to aid future development of foundation models for\ncomputer vision. We note that SA-1B will be released un-\nder a favorable license agreement for certain research uses\nand with protections for researchers.\nImages.\nWe licensed a new set of 11M images from a', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='and with protections for researchers.\nImages.\nWe licensed a new set of 11M images from a\nprovider that works directly with photographers. These im-\nages are high resolution (3300⇥4950 pixels on average),\nand the resulting data size can present accessibility and stor-\nage challenges. Therefore, we are releasing downsampled\nimages with their shortest side set to 1500 pixels. Even af-\nter downsampling, our images are signiﬁcantly higher reso-\nlution than many existing vision datasets (e.g., COCO [64]\nimages are ⇠480⇥640 pixels). Note that most models today\noperate on much lower resolution inputs. Faces and vehicle\nlicense plates have been blurred in the released images.', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='operate on much lower resolution inputs. Faces and vehicle\nlicense plates have been blurred in the released images.\nMasks. Our data engine produced 1.1B masks, 99.1% of\nwhich were generated fully automatically. Therefore, the\nquality of the automatic masks is centrally important. We\ncompare them directly to professional annotations and look\nat how various mask properties compare to prominent seg-\nmentation datasets. Our main conclusion, as borne out in\nthe analysis below and the experiments in §6, is that our\nautomatic masks are high quality and effective for training\nmodels. Motivated by these ﬁndings, SA-1B only includes\nautomatically generated masks.\nMask quality. To estimate mask quality, we randomly sam-', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='models. Motivated by these ﬁndings, SA-1B only includes\nautomatically generated masks.\nMask quality. To estimate mask quality, we randomly sam-\npled 500 images (⇠50k masks) and asked our professional\nannotators to improve the quality of all masks in these im-\nages. Annotators did so using our model and pixel-precise\n“brush” and “eraser” editing tools. This procedure resulted\nin pairs of automatically predicted and professionally cor-\nrected masks. We computed IoU between each pair and\nfound that 94% of pairs have greater than 90% IoU (and\n97% of pairs have greater than 75% IoU). For comparison,\nprior work estimates inter-annotator consistency at 85-91%', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='97% of pairs have greater than 75% IoU). For comparison,\nprior work estimates inter-annotator consistency at 85-91%\nIoU [43, 58]. Our experiments in §6 conﬁrm by human rat-\nings that mask quality is high relative to a variety of datasets\nand that training our model on automatic masks is nearly as\ngood as using all masks produced by the data engine.\n4020\n\x18\x0e\x04\x07\x0f\n\x07\x07\x16\x01\x1d\x1f\x1a\x1c\x1b"\n\x07\x07\x08\r\x16\x01\x02\x07\x05\x07\x0f\x03\x01\x1f\x1a"\x1e"\n\x15\x19\x13\x18\x01#\x07\n\x06\x05\x07\x08\x06\x16\x01\x1d\x1f\x1a\x1c\x1b"\n\x07\x05\n\x16\x01\x1f\x1a"\x1e"', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='\x06\x05\x07\x08\x06\x16\x01\x1d\x1f\x1a\x1c\x1b"\n\x07\x05\n\x16\x01\x1f\x1a"\x1e"\n\x10\x17\x10\x17\n\x06\x05\x07\x08\t\x16\x01\x1d\x1f\x1a\x1c\x1b"\n\x06\x05\r\x16\x01\x1f\x1a"\x1e"\n\x0e\x11\x12\x08\x06\x14\n\x06\x05\x06\x08\x0c\x16\x01\x1d\x1f\x1a\x1c\x1b"\n\x06\x05\x0b\x16\x01\x1f\x1a"\x1e"\n\x17!\x1b \x01\x13\x1f\x1a\x1c\x1b"\n\x07\x16\x01\x1d\x1f\x1a\x1c\x1b"\n\x08\x05\x0b\x16\x01\x1f\x1a"\x1e"\n\x04\x03\n\x04\x04\x02\x07\x03\n\x07\x04\x02\x04\x03\x03\n\x04\x03\x04\x02\x05\x03\x03\n\x05\x03\x03', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='\x04\x04\x02\x07\x03\n\x07\x04\x02\x04\x03\x03\n\x04\x03\x04\x02\x05\x03\x03\n\x05\x03\x03\n\x15\x0e\x10\x19\x01\x17\x11\x01\x15\r\x1a\x14\x1a\x01\x18\x10\x19\x01\x13\x15\r\x12\x10\n\x03\n\x06\x03\n\x08\x03\n\x10\x19\x0f\x10\x16\x1b\x01\x17\x11\x01\x13\x15\r\x12\x10\x1a\n\x03\x02\x03\x03\n\x03\x02\x05\x06\n\x03\x02\x06\x03\n\x03\x02\x07\x06\n\x11\n\x17\x0f\x18\x0c\x01\x16\x0c\x0e\x12\x0c\x13\x17\n\x17\x0f\x14\x13\x01\x12\n\x16\x10\x01\x16\x0f\x19\n\x04\x03\x03\n\x04\x03\x1c\x05', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='\x17\x0f\x14\x13\x01\x12\n\x16\x10\x01\x16\x0f\x19\n\x04\x03\x03\n\x04\x03\x1c\x05\n\x08\x0c\x15\x0b\x0c\x13\x17\x01\x14\r\x01\x12\n\x16\x10\x16\n\x03\x02\x03\n\x03\x02\x05\n\x03\x02\x06\n\x03\x02\x08\n\x03\x02\n\x14\x13\r\x0c\x18\x10\x17\x19\n\x03\n\x07\n\x04\x03\n\x04\x07\n\x0e\x15\r\x0e\x13\x17\x01\x14\x0f\x01\x12\x0c\x16\x11\x16\nFigure 6: Dataset mask properties. The legend references the number of images and masks in each dataset. Note, that SA-1B', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='Figure 6: Dataset mask properties. The legend references the number of images and masks in each dataset. Note, that SA-1B\nhas 11⇥ more images and 400⇥ more masks than the largest existing segmentation dataset Open Images [58].\nMask properties. In Fig. 5 we plot the spatial distribution\nof object centers in SA-1B compared to the largest existing\nsegmentation datasets. Common photographer biases are\npresent in all datasets. We observe that SA-1B has greater\ncoverage of image corners compared to LVIS v1 [43] and\nADE20K [115], the two most similarly distributed datasets,\nwhile COCO [64] and Open Images V5 [58] have a more\nprominent center bias. In Fig. 6 (legend) we compare these', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='while COCO [64] and Open Images V5 [58] have a more\nprominent center bias. In Fig. 6 (legend) we compare these\ndatasets by size. SA-1B has 11⇥ more images and 400⇥\nmore masks than the second largest, Open Images. On av-\nerage, it has 36⇥ more masks per image than Open Images.\nThe closest dataset in this respect, ADE20K, still has 3.5⇥\nfewer masks per image. Fig. 6 (left) plots the masks-per-\nimage distribution. Next, we look at image-relative mask\nsize (square root of the mask area divided by image area)\nin Fig. 6 (middle). As expected, since our dataset has more', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='size (square root of the mask area divided by image area)\nin Fig. 6 (middle). As expected, since our dataset has more\nmasks per image, it also tends to include a greater percent-\nage of small and medium relative-size masks. Finally, to\nanalyze shape complexity, we look at mask concavity (1\nminus mask area divided by area of mask’s convex hull) in\nFig. 6 (right). Since shape complexity is correlated with\nmask size, we control for the datasets’ mask size distribu-\ntions by ﬁrst performing stratiﬁed sampling from binned\nmask sizes. We observe that the concavity distribution of\nour masks is broadly similar to that of other datasets.\n6. Zero-Shot Transfer Experiments\nIn this section, we present zero-shot transfer experiments', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='our masks is broadly similar to that of other datasets.\n6. Zero-Shot Transfer Experiments\nIn this section, we present zero-shot transfer experiments\nwith SAM, the Segment Anything Model. We consider ﬁve\ntasks, four of which differ signiﬁcantly from the promptable\nsegmentation task used to train SAM. These experiments\nevaluate SAM on datasets and tasks that were not seen dur-\ning training (our usage of “zero-shot transfer” follows its\nusage in CLIP [80]). The datasets may include novel image\ndistributions, such as underwater or ego-centric images that,\nto our knowledge, do not appear in SA-1B.\nOur experiments begin by testing the core goal of\npromptable segmentation: producing a valid mask from any\nprompt. We emphasize the challenging scenario of a single', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='Our experiments begin by testing the core goal of\npromptable segmentation: producing a valid mask from any\nprompt. We emphasize the challenging scenario of a single\nforeground point prompt, since it is more likely to be am-\nbiguous than other more speciﬁc prompts. Next, we present\na sequence of experiments that traverse low, mid, and high-\nlevel image understanding and roughly parallel the histori-\ncal development of the ﬁeld. Speciﬁcally, we prompt SAM\nto (1) perform edge detection, (2) segment everything, i.e.\nobject proposal generation, (3) segment detected objects,\ni.e. instance segmentation, and (4), as a proof-of-concept,\nto segment objects from free-form text. These four tasks', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='i.e. instance segmentation, and (4), as a proof-of-concept,\nto segment objects from free-form text. These four tasks\ndiffer signiﬁcantly from the promptable segmentation task\nthat SAM was trained on and are implemented via prompt\nengineering. We report zero-shot single point valid mask\nevaluation and zero-shot text to mask proof-of-concept in\nthe main text. We refer readers to the supplement for our\nexperiments with zero-shot edge detection, object proposal,\nand instance segmentation. In addition, we report a set of\nablations in the supplement. We analyze SAM performance\nwith respect to the size and composition of its training data\nas well as the image encoder architecture.\nImplementation.\nUnless otherwise speciﬁed: (1) SAM', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='as well as the image encoder architecture.\nImplementation.\nUnless otherwise speciﬁed: (1) SAM\nuses an MAE [46] pre-trained ViT-H [32] image encoder\nand (2) SAM was trained on SA-1B, noting that this dataset\nincludes only automatically generated masks from the ﬁnal\nstage of our data engine. For all other model and training\ndetails, such as hyperparameters, refer to §B.\n6.1. Zero-Shot Single Point Valid Mask Evaluation\nTask. We evaluate segmenting an object from a single fore-\nground point. This task is ill-posed as one point can refer\nto multiple objects. Ground truth masks in most datasets\ndo not enumerate all possible masks, which can make au-', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='to multiple objects. Ground truth masks in most datasets\ndo not enumerate all possible masks, which can make au-\ntomatic metrics unreliable. Therefore, we supplement the\nstandard mIoU metric (i.e., the mean of all IoUs between\npredicted and ground truth masks) with a human study in\nwhich annotators rate mask quality from 1 (nonsense) to 10\n(pixel-perfect). See §E.1, §F, and §H for additional details.\nBy default, we sample points from the “center” of ground\ntruth masks (at a maximal value of the mask’s interior dis-\ntance transform), following the standard evaluation proto-\ncol in interactive segmentation [90]. Since SAM is capable\nof predicting multiple masks, we evaluate only the model’s', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='col in interactive segmentation [90]. Since SAM is capable\nof predicting multiple masks, we evaluate only the model’s\nmost conﬁdent mask by default.\nThe baselines are all\nsingle-mask methods. We compare mainly to RITM [90],\na strong interactive segmenter that performs best on our\nbenchmark compared to other strong baselines [65, 17].\n4021\nADE20K [115]\nBBBC038v1 [11]\nCityscapes [24]\nDOORS [78]\nDRAM [23]\nEgoHOS [111]\nGTEA [33, 61]\nHypersim [84]', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='EgoHOS [111]\nGTEA [33, 61]\nHypersim [84]\nIBD [16]\niShape [109]\nLVIS [43]\nNDD20 [98]\nNDISPark [21, 22]\nOVIS [79]\nPPDLS [72]\nPlittersdorf [45]\nSTREETS [89]\nTimberSeg [37]\nTrashCan [50]\nVISOR [27, 26]\nWoodScape [110]\nPIDRay [102]', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='VISOR [27, 26]\nWoodScape [110]\nPIDRay [102]\nZeroWaste-f [6]\n(a) Samples from the 23 diverse segmentation datasets used to evaluate SAM’s zero-shot transfer capabilities\n20\n0\n+20\n+40\nIoU delta at 1 center point\nGTEA [33, 61]\nTrashCan [50]\nDRAM [23]\nPIDRay [102]\nCityscapes [24]\nWoodScape [110]\nIBD [16]\nEgoHOS [111]', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='WoodScape [110]\nIBD [16]\nEgoHOS [111]\nPlittersdorf [45]\nVISOR [27, 26]\nNDISPark [21, 22]\nHypersim [84]\nOVIS [79]\nADE20K [115]\niShape [109]\nZeroWaste-f [6]\nSTREETS [89]\nLVIS [43]\nNDD20 [98]\nTimberSeg [37]\nDOORS [78]\nBBBC038v1 [11]', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='TimberSeg [37]\nDOORS [78]\nBBBC038v1 [11]\nPPDLS [72]\n21.4\n15.0\n6.5\n5.8\n2.0\n0.6\n0.3\n+0.8\n+1.5\n+1.8\n+2.7\n+6.1\n+7.0\n+7.8\n+8.8\n+9.1\n+17.3\n+18.5\n+21.1', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='+9.1\n+17.3\n+18.5\n+21.1\n+28.9\n+41.1\n+44.7\n+46.9\n(b) SAM vs. RITM [90] on 23 datasets\n\x0e\x15\r\x13\n\x15\r\x13\x11\x12\n\x12\t\x0f\n\x10\x0b\x0b\x05\x04\n\x11\x15\r\x13\n\x1b\x13\x1a\x16!\x18\n\x16$\x16#\x18$#\n\x06\n\x07\n\x08\n&\x19\x03\x01\x1e\x16#\x1c\x01"\x16$\x1b\x1f\x19', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='\x06\n\x07\n\x08\n&\x19\x03\x01\x1e\x16#\x1c\x01"\x16$\x1b\x1f\x19\n" %\x1f\x17\x01\x14"%$\x1a\n\x13\t\x0f\n\x13\t\x0f\x01\x02\x01#\x1b\x1f\x19\x1d\x18\x01 %$!%$\n\x12\r\x14\x0f\n(c) Mask quality ratings by human annotators\n\x05\n\x06\n\x07\n\x08\n\x10%\x1e\x16\x19"\x01 \x1a\x01! \x1b\x1f$#\n\x08\x04\n\x08\n\x0e \x14\x01\x02\x06\x07\x01\x18\x15$\x15#\x19$#\x03\n\x12\x0b\x0f\x01\x02 "\x15\x17\x1d\x19\x03\n\x12\x0b\x0f', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='\x0e \x14\x01\x02\x06\x07\x01\x18\x15$\x15#\x19$#\x03\n\x12\x0b\x0f\x01\x02 "\x15\x17\x1d\x19\x03\n\x12\x0b\x0f\n\x11\x0e\x13\x0f\n\x12\x1b\x1e! \x19\x0c\x1d\x1b\x17\n\x17\x15\x1d\x0c\x1d\x1b\x17\n\x05\n\x06\n\x07\n\x08\n\x0e \x19\x12\x15\x1d\x01\x1b\x16\x01\x1c\x1b\x17\x1a\n\x08\x04\n\x08\n\x19\x0c\x1b\x10\x01\x02\x06\x07\x01\x14\x11\x1f\x11\x1e\x15\x1f\x1e\x03\n\x0f\x0b\r\x01\x02\x1b\x1d\x11\x13\x18\x15\x03\n(d) Center points (default)\n(e) Random points', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='\x0f\x0b\r\x01\x02\x1b\x1d\x11\x13\x18\x15\x03\n(d) Center points (default)\n(e) Random points\nFigure 7: Point to mask evaluation on 23 datasets. (a) Dataset samples. (b) Mean IoU of SAM and the strongest single point\nsegmenter, RITM [90]. Due to ambiguity, a single mask may not match ground truth; circles show “oracle” results of the\nmost relevant of SAM’s 3 predictions. (c) Per-dataset comparison of mask quality ratings by annotators from 1 (worst) to 10\n(best). Mask center is used as the prompt. (d, e) mIoU with varying number of points. SAM signiﬁcantly outperforms prior\ninteractive segmenters with 1 point and is on par with more points. Low absolute mIoU at 1 point is the result of ambiguity.', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='interactive segmenters with 1 point and is on par with more points. Low absolute mIoU at 1 point is the result of ambiguity.\nDatasets. We use a newly compiled suite of 23 datasets\nwith diverse image distributions, see appendix Table 4 for\nmore details. We use all 23 datasets for mIoU evaluation.\nFor the human study, we use the subset listed in Fig. 7c\n(due to the resource requirements of such studies). This\nsubset includes both datasets for which SAM outperforms\nand underperforms RITM according to automatic metrics.\nResults. First, we look at automatic evaluation on the full\nsuite of 23 datasets using mIoU. We compare per-dataset\nresults in Fig. 7b against RITM. SAM yields higher re-', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='suite of 23 datasets using mIoU. We compare per-dataset\nresults in Fig. 7b against RITM. SAM yields higher re-\nsults on 16 of the 23 datasets, by as much as ⇠47 IoU. We\nalso present an “oracle” result, in which the most relevant\nof SAM’s 3 masks is selected by comparing them to the\nground truth, rather than selecting the most conﬁdent mask.\nThis reveals the impact of ambiguity on automatic evalu-\nation. In particular, with the oracle to perform ambiguity\nresolution, SAM outperforms RITM on all datasets.\nResults of the human study are presented in Fig. 7c. Er-\nror bars are 95% conﬁdence intervals (all differences are', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='Results of the human study are presented in Fig. 7c. Er-\nror bars are 95% conﬁdence intervals (all differences are\nsigniﬁcant; see §F for details). We observe that the annota-\ntors consistently rate the quality of SAM’s masks substan-\ntially higher than the strongest baseline, RITM. An ablated,\n“ambiguity-unaware” version of SAM with a single output\nmask has consistently lower ratings. SAM’s mean ratings\nfall between 7 and 9, which corresponds to the qualitative\nrating guideline: “A high score (7-9): The object is identi-\nﬁable and errors are small and rare (e.g., missing a small,\nheavily obscured disconnected component, ...).” These re-', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='ﬁable and errors are small and rare (e.g., missing a small,\nheavily obscured disconnected component, ...).” These re-\nsults indicate that SAM has learned to segment valid masks\nfrom a single point. Note that for datasets like DRAM and\nIBD, where SAM is worse on automatic metrics, it receives\nconsistently higher ratings in the human study.\nFig. 7d shows additional baselines, SimpleClick [65] and\nFocalClick [17]. As the number of points increases from 1\nto 9, we observe that the gap between methods decreases.\nThis is expected as the task becomes easier; also, SAM is\nnot optimized for the very high IoU regime. Finally, in\nFig. 7e we replace the default center point sampling with\nrandom point sampling. We observe that the gap between', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='Fig. 7e we replace the default center point sampling with\nrandom point sampling. We observe that the gap between\nSAM and the baselines grows and SAM is able to achieve\ncomparable results under either sampling method.\n4022\n\x01\x01\x01\x01\x01\x07\x02\x01\x06\x04\x03\x03\x05\x08\n3\n\x01\x01\x01\x01\x01\r\x03\x04\x02\x0c\x04\n\x01\x0b\t\t\x0b\x06\x01\x05\n\x07\x08\x08\x04\x0e\n3\n\x01\x01\x01\x01\x01\x08\x02\x01\x07\x04\x05\x03\x06\n7\n\x01\x01\x01\x01\x01\x0c\x03\x01\x0b\x05\x08\x04\t\r\x01\x02\x01\x08\x07\x05\x06\n3\n\x01\x01\x01\x01\x01\x08\x07\x03\x04\x02\x05\x06\n7', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='3\n\x01\x01\x01\x01\x01\x08\x07\x03\x04\x02\x05\x06\n7\n\x01\x01\x01\x01\x01\x0c\x0b\x04\x07\x03\x08\t\r\x01\x02\x01\x07\x06\x04\x05\n3\nFigure 8: Zero-shot text-to-mask. SAM can work with sim-\nple and nuanced text prompts. When SAM fails to make a\ncorrect prediction, an additional point prompt can help.\n6.2. Zero-Shot Text-to-Mask\nApproach.\nThis experiment is a proof-of-concept of\nSAM’s ability to segment objects from free-form text\nprompts. While we used the exact same SAM in all prior\nexperiments, for this one SAM’s training procedure is mod-', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='prompts. While we used the exact same SAM in all prior\nexperiments, for this one SAM’s training procedure is mod-\niﬁed to make it text-aware, but in a way that does not require\nnew text annotations. Speciﬁcally, for each manually col-\nlected mask with area larger than 1002 we extract the CLIP\nimage embedding. Then, during training, we prompt SAM\nwith the extracted CLIP image embeddings as its ﬁrst in-\nteraction. The key observation here is that because CLIP’s\nimage embeddings are trained to align with its text embed-\ndings, we can train with image embeddings, but use text\nembeddings for inference. That is, at inference time we run\ntext through CLIP’s text encoder and then give the resulting', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='embeddings for inference. That is, at inference time we run\ntext through CLIP’s text encoder and then give the resulting\ntext embedding as a prompt to SAM (see §E.5 for details).\nResults.\nWe show qualitative results in Fig. 8.\nSAM\ncan segment objects based on simple text prompts like “a\nwheel” as well as phrases like “beaver tooth grille”. When\nSAM fails to pick the right object from a text prompt only,\nan additional point often ﬁxes the prediction, similar to [30].\n7. Discussion\nFoundation models. Pre-trained models have been adapted\nto downstream tasks since the early days of machine learn-\ning [97]. This paradigm has become increasingly impor-', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='Foundation models. Pre-trained models have been adapted\nto downstream tasks since the early days of machine learn-\ning [97]. This paradigm has become increasingly impor-\ntant in recent years with a growing emphasis on scale, and\nsuch models have recently been (re-)branded as “founda-\ntion models”: i.e. models that are “trained on broad data\nat scale and are adaptable to a wide range of downstream\ntasks” [8]. Our work correlates well with this deﬁnition,\nthough we note that a foundation model for image segmen-\ntation is an inherently limited scope, since it represents an\nimportant, yet fractional, subset of computer vision. We\nalso contrast one aspect of our approach with [8], which\nemphasizes the role of self-supervised learning in founda-', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='also contrast one aspect of our approach with [8], which\nemphasizes the role of self-supervised learning in founda-\ntion models. While our model is initialized with a self-\nsupervised technique (MAE [46]), the vast majority of its\ncapabilities come from large-scale supervised training. In\ncases where data engines can scale available annotations,\nlike ours, supervised training provides an effective solution.\nCompositionality. Pre-trained models can power new ca-\npabilities even beyond ones imagined at the moment of\ntraining. One prominent example is how CLIP [80] is used\nas a component in larger systems, such as DALL·E [81].\nOur goal is to make this kind of composition straightfor-\nward with SAM. We aim to achieve this by requiring SAM', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='Our goal is to make this kind of composition straightfor-\nward with SAM. We aim to achieve this by requiring SAM\nto predict a valid mask for a wide range of segmentation\nprompts. The effect is to create a reliable interface between\nSAM and other components. For example, MCC [104] can\neasily use SAM to segment an object of interest and achieve\nstrong generalization to unseen objects for 3D reconstruc-\ntion from a single RGB-D image. In another example, SAM\ncan be prompted with gaze points detected by a wearable\ndevice, enabling new applications. Thanks to SAM’s abil-\nity to generalize to new domains like ego-centric images,\nsuch systems work without need for additional training.\nLimitations. While SAM performs well in general, it is', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='such systems work without need for additional training.\nLimitations. While SAM performs well in general, it is\nnot perfect. It can miss ﬁne structures, hallucinates small\ndisconnected components at times, and does not produce\nboundaries as crisply as more computationally intensive\nmethods that “zoom-in”, e.g. [17]. In general, we expect\ndedicated interactive segmentation methods to outperform\nSAM when many points are provided, e.g. [65]. Unlike\nthese methods, SAM is designed for generality and breadth\nof use rather than high IoU interactive segmentation. More-\nover, SAM can process prompts in real-time, but neverthe-\nless SAM’s overall performance is not real-time when using\na heavy image encoder. Our foray into the text-to-mask task', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='less SAM’s overall performance is not real-time when using\na heavy image encoder. Our foray into the text-to-mask task\nis exploratory and not entirely robust, although we believe\nit can be improved with more effort. While SAM can per-\nform many tasks, it is unclear how to design simple prompts\nthat implement semantic and panoptic segmentation. Fi-\nnally, there are domain-speciﬁc tools, such as [7], that we\nexpect to outperform SAM in their respective domains.\nConclusion. The Segment Anything project is an attempt to\nlift image segmentation into the era of foundation models.\nOur principal contributions are a new task (promptable seg-\nmentation), model (SAM), and dataset (SA-1B) that make', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='Our principal contributions are a new task (promptable seg-\nmentation), model (SAM), and dataset (SA-1B) that make\nthis leap possible. Whether SAM achieves the status of a\nfoundation model remains to be seen by how it is used in\nthe community, but regardless we expect the perspective of\nthis work, the release of over 1B masks, and our promptable\nsegmentation model will help pave the path ahead.\n4023\nReferences\n[1] Edward H Adelson. On seeing stuff: the perception of materials by\nhumans and machines. Human vision and electronic imaging VI,\n2001. 5\n[2] Bogdan Alexe, Thomas Deselaers, and Vittorio Ferrari. What is an', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='2001. 5\n[2] Bogdan Alexe, Thomas Deselaers, and Vittorio Ferrari. What is an\nobject? CVPR, 2010. 4, 19\n[3] Pablo Arbel´aez, Michael Maire, Charless Fowlkes, and Jitendra\nMalik.\nContour detection and hierarchical image segmentation.\nTPAMI, 2010. 4, 19, 28\n[4] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer\nnormalization. arXiv:1607.06450, 2016. 13\n[5] Hangbo Bao, Li Dong, and Furu Wei. BEiT: BERT pre-training of', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='[5] Hangbo Bao, Li Dong, and Furu Wei. BEiT: BERT pre-training of\nimage transformers. arXiv:2106.08254, 2021. 15\n[6] Dina Bashkirova, Mohamed Abdelfattah, Ziliang Zhu, James Akl,\nFadi Alladkani, Ping Hu, Vitaly Ablavsky, Berk Calli, Sarah Adel\nBargal, and Kate Saenko. ZeroWaste dataset: Towards deformable\nobject segmentation in cluttered scenes. CVPR, 2022. 8, 18\n[7] Stuart Berg, Dominik Kutra, Thorben Kroeger, Christoph N.\nStraehle, Bernhard X. Kausler, Carsten Haubold, Martin Schiegg,', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='Straehle, Bernhard X. Kausler, Carsten Haubold, Martin Schiegg,\nJanez Ales, Thorsten Beier, Markus Rudy, Kemal Eren, Jaime I.\nCervantes, Buote Xu, Fynn Beuttenmueller, Adrian Wolny, Chong\nZhang, Ullrich Koethe, Fred A. Hamprecht, and Anna Kreshuk.\nilastik: interactive machine learning for (bio)image analysis. Na-\nture Methods, 2019. 9\n[8] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman,\nSimran Arora, Sydney von Arx, Michael S Bernstein, Jeannette\nBohg, Antoine Bosselut, Emma Brunskill, et al. On the opportu-', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportu-\nnities and risks of foundation models. arXiv:2108.07258, 2021. 1,\n9\n[9] Gustav Bredell, Christine Tanner, and Ender Konukoglu. Iterative\ninteraction training for segmentation editing networks. MICCAI,\n2018. 15\n[10] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,\nJared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav\nShyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel\nHerbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child,', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris\nHesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Ben-\njamin Chess, Jack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei. Language models\nare few-shot learners. NeurIPS, 2020. 1, 4\n[11] Juan C. Caicedo, Allen Goodman, Kyle W. Karhohs, Beth A. Ci-\nmini, Jeanelle Ackerman, Marzieh Haghighi, CherKeng Heng, Tim\nBecker, Minh Doan, Claire McQuin, Mohammad Rohban, Shan-', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='Becker, Minh Doan, Claire McQuin, Mohammad Rohban, Shan-\ntanu Singh, and Anne E. Carpenter. Nucleus segmentation across\nimaging experiments: the 2018 data science bowl. Nature Methods,\n2019. 8, 17, 18\n[12] John Canny. A computational approach to edge detection. TPAMI,\n1986. 19\n[13] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end\nobject detection with Transformers. ECCV, 2020. 5, 13, 14\n[14] Guillaume Charpiat, Matthias Hofmann, and Bernhard Sch¨olkopf.', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='[14] Guillaume Charpiat, Matthias Hofmann, and Bernhard Sch¨olkopf.\nAutomatic image colorization via multimodal predictions. ECCV,\n2008. 5, 14\n[15] Neelima Chavali, Harsh Agrawal, Aroma Mahendru, and Dhruv\nBatra. Object-proposal evaluation protocol is’ gameable’. CVPR,\n2016. 19, 20\n[16] Jiazhou Chen, Yanghui Xu, Shufang Lu, Ronghua Liang, and Lian-\ngliang Nan. 3D instance segmentation of MVS buildings. IEEE\nTransactions on Geoscience and Remote Sensing, 2022. 8, 17, 18,', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='Transactions on Geoscience and Remote Sensing, 2022. 8, 17, 18,\n22, 23, 24\n[17] Xi Chen, Zhiyan Zhao, Yilei Zhang, Manni Duan, Donglian Qi, and\nHengshuang Zhao. FocalClick: towards practical interactive image\nsegmentation. CVPR, 2022. 7, 8, 9, 17, 19\n[18] Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kir-\nillov, and Rohit Girdhar. Masked-attention mask transformer for\nuniversal image segmentation. CVPR, 2022. 4\n[19] Bowen Cheng, Alex Schwing, and Alexander Kirillov.\nPer-', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='[19] Bowen Cheng, Alex Schwing, and Alexander Kirillov.\nPer-\npixel classiﬁcation is not all you need for semantic segmentation.\nNeurIPS, 2021. 5, 13, 14\n[20] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten\nBosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won\nChung, Charles Sutton, Sebastian Gehrmann, et al. PaLM: Scaling\nlanguage modeling with pathways. arXiv:2204.02311, 2022. 1\n[21] Luca Ciampi, Carlos Santiago, Joao Costeira, Claudio Gennaro, and', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='[21] Luca Ciampi, Carlos Santiago, Joao Costeira, Claudio Gennaro, and\nGiuseppe Amato. Domain adaptation for trafﬁc density estimation.\nInternational Joint Conference on Computer Vision, Imaging and\nComputer Graphics Theory and Applications, 2021. 8, 18\n[22] Luca Ciampi, Carlos Santiago, Joao Costeira, Claudio Gennaro, and\nGiuseppe Amato. Night and day instance segmented park (NDIS-\nPark) dataset: a collection of images taken by day and by night for\nvehicle detection, segmentation and counting in parking areas. Zen-\nodo, 2022. 8, 18\n[23] Nadav Cohen, Yael Newman, and Ariel Shamir. Semantic segmen-', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='odo, 2022. 8, 18\n[23] Nadav Cohen, Yael Newman, and Ariel Shamir. Semantic segmen-\ntation in art paintings. Computer Graphics Forum, 2022. 8, 17, 18,\n22, 23, 24\n[24] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld,\nMarkus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth,\nand Bernt Schiele. The Cityscapes dataset for semantic urban scene\nunderstanding. CVPR, 2016. 8, 17, 18\n[25] Bruno da Silva, George Konidaris, and Andrew Barto. Learning', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='understanding. CVPR, 2016. 8, 17, 18\n[25] Bruno da Silva, George Konidaris, and Andrew Barto. Learning\nparameterized skills. ICML, 2012. 4\n[26] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Antonino\nFurnari, Jian Ma, Evangelos Kazakos, Davide Moltisanti, Jonathan\nMunro, Toby Perrett, Will Price, and Michael Wray.\nRescaling\negocentric vision: Collection, pipeline and challenges for EPIC-\nKITCHENS-100. IJCV, 2022. 8, 18, 22, 23, 24', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='KITCHENS-100. IJCV, 2022. 8, 18, 22, 23, 24\n[27] Ahmad Darkhalil, Dandan Shan, Bin Zhu, Jian Ma, Amlan Kar,\nRichard Higgins, Sanja Fidler, David Fouhey, and Dima Damen.\nEPIC-KITCHENS VISOR benchmark: Video segmentations and\nobject relations. NeurIPS, 2022. 8, 17, 18, 22, 23, 24\n[28] Terrance De Vries, Ishan Misra, Changhan Wang, and Laurens\nVan der Maaten. Does object recognition work for everyone? CVPR\nworkshops, 2019. 16', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='Van der Maaten. Does object recognition work for everyone? CVPR\nworkshops, 2019. 16\n[29] Mark D´ıaz, Ian Kivlichan, Rachel Rosen, Dylan Baker, Razvan\nAmironesei, Vinodkumar Prabhakaran, and Emily Denton. Crowd-\nWorkSheets: Accounting for individual and collective identities un-\nderlying crowdsourced dataset annotation.\nACM Conference on\nFairness, Accountability, and Transparency, 2022. 24\n[30] Henghui Ding, Scott Cohen, Brian Price, and Xudong Jiang.\nPhraseClick: toward achieving ﬂexible interactive segmentation by\nphrase and click. ECCV, 2020. 9', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='PhraseClick: toward achieving ﬂexible interactive segmentation by\nphrase and click. ECCV, 2020. 9\n[31] Piotr Doll´ar and C Lawrence Zitnick. Fast edge detection using\nstructured forests. TPAMI, 2014. 19\n[32] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk\nWeissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa De-\nhghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob\nUszkoreit, and Neil Houlsby.\nAn image is worth 16x16 words:\nTransformers for image recognition at scale. ICLR, 2021. 5, 7,\n13', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='Transformers for image recognition at scale. ICLR, 2021. 5, 7,\n13\n[33] Alireza Fathi, Xiaofeng Ren, and James M. Rehg. Learning to rec-\nognize objects in egocentric activities. CVPR, 2011. 8, 17, 18\n[34] Pedro F Felzenszwalb and Daniel P Huttenlocher. Efﬁcient graph-\nbased image segmentation. IJCV, 2004. 19\n[35] Thomas B. Fitzpatrick. The validity and practicality of sun-reactive\nskin types i through vi. Archives of Dermatology, 1988. 17\n[36] Marco Forte, Brian Price, Scott Cohen, Ning Xu, and Franc¸ois', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='[36] Marco Forte, Brian Price, Scott Cohen, Ning Xu, and Franc¸ois\nPiti´e.\nGetting to 99% accuracy in interactive segmentation.\narXiv:2003.07932, 2020. 5, 14, 15\n[37] Jean-Michel Fortin, Olivier Gamache, Vincent Grondin, Franc¸ois\nPomerleau, and Philippe Gigu`ere. Instance segmentation for au-\ntonomous log grasping in forestry operations. IROS, 2022. 8, 18\n[38] Timnit Gebru,\nJamie Morgenstern,\nBriana Vecchione,\nJen-', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='[38] Timnit Gebru,\nJamie Morgenstern,\nBriana Vecchione,\nJen-\nnifer Wortman Vaughan, Hanna Wallach, Hal Daum´e Iii, and Kate\n4024\nCrawford. Datasheets for datasets. Communications of the ACM,\n2021. 24\n[39] Golnaz Ghiasi, Yin Cui, Aravind Srinivas, Rui Qian, Tsung-Yi Lin,\nEkin D Cubuk, Quoc V Le, and Barret Zoph. Simple copy-paste is a\nstrong data augmentation method for instance segmentation. CVPR,\n2021. 13, 15, 21', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='strong data augmentation method for instance segmentation. CVPR,\n2021. 13, 15, 21\n[40] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik.\nRich feature hierarchies for accurate object detection and semantic\nsegmentation. CVPR, 2014. 19\n[41] Priya Goyal, Piotr Doll´ar, Ross Girshick, Pieter Noordhuis, Lukasz\nWesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and\nKaiming He. Accurate, large minibatch SGD: Training ImageNet\nin 1 hour. arXiv:1706.02677, 2017. 15', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='in 1 hour. arXiv:1706.02677, 2017. 15\n[42] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary\nChavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger,\nHao Jiang, Miao Liu, Xingyu Liu, Miguel Martin, Tushar Na-\ngarajan, Ilija Radosavovic, Santhosh Kumar Ramakrishnan, Fiona\nRyan, Jayant Sharma, Michael Wray, Mengmeng Xu, Eric Zhong-\ncong Xu, Chen Zhao, Siddhant Bansal, Dhruv Batra, Vincent Car-\ntillier, Sean Crane, Tien Do, Morrie Doulaty, Akshay Erapalli,', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='tillier, Sean Crane, Tien Do, Morrie Doulaty, Akshay Erapalli,\nChristoph Feichtenhofer, Adriano Fragomeni, Qichen Fu, Chris-\ntian Fuegen, Abrham Gebreselasie, Cristina Gonzalez, James Hillis,\nXuhua Huang, Yifei Huang, Wenqi Jia, Weslie Khoo, Jachym Ko-\nlar, Satwik Kottur, Anurag Kumar, Federico Landini, Chao Li,\nYanghao Li, Zhenqiang Li, Karttikeya Mangalam, Raghava Mod-\nhugu, Jonathan Munro, Tullie Murrell, Takumi Nishiyasu, Will\nPrice, Paola Ruiz Puentes, Merey Ramazanova, Leda Sari, Kiran', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='Price, Paola Ruiz Puentes, Merey Ramazanova, Leda Sari, Kiran\nSomasundaram, Audrey Southerland, Yusuke Sugano, Ruijie Tao,\nMinh Vo, Yuchen Wang, Xindi Wu, Takuma Yagi, Yunyi Zhu,\nPablo Arbelaez, David Crandall, Dima Damen, Giovanni Maria\nFarinella, Bernard Ghanem, Vamsi Krishna Ithapu, C. V. Jawahar,\nHanbyul Joo, Kris Kitani, Haizhou Li, Richard Newcombe, Aude\nOliva, Hyun Soo Park, James M. Rehg, Yoichi Sato, Jianbo Shi,\nMike Zheng Shou, Antonio Torralba, Lorenzo Torresani, Mingfei', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='Mike Zheng Shou, Antonio Torralba, Lorenzo Torresani, Mingfei\nYan, and Jitendra Malik. Ego4D: Around the World in 3,000 Hours\nof Egocentric Video. CVPR, 2022. 18\n[43] Agrim Gupta, Piotr Dollar, and Ross Girshick. LVIS: A dataset for\nlarge vocabulary instance segmentation. CVPR, 2019. 2, 6, 7, 8, 17,\n18, 19, 21, 23\n[44] Abner Guzman-Rivera, Dhruv Batra, and Pushmeet Kohli. Multiple\nchoice learning: Learning to produce multiple structured outputs.\nNeurIPS, 2012. 5, 14', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='choice learning: Learning to produce multiple structured outputs.\nNeurIPS, 2012. 5, 14\n[45] Timm\nHaucke,\nHjalmar\nS.\nK¨uhl,\nand\nVolker\nSteinhage.\nSOCRATES: Introducing depth in visual wildlife monitoring using\nstereo vision. Sensors, 2022. 8, 18\n[46] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll´ar,\nand Ross Girshick. Masked autoencoders are scalable vision learn-\ners. CVPR, 2022. 5, 7, 9, 13, 15', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='ers. CVPR, 2022. 5, 7, 9, 13, 15\n[47] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep\nresidual learning for image recognition. CVPR, 2016. 14\n[48] Dan Hendrycks and Kevin Gimpel.\nGaussian error linear units\n(gelus). arXiv:1606.08415, 2016. 13\n[49] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena\nBuchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas,\nLisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas,\nLisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training\ncompute-optimal large language models. arXiv:2203.15556, 2022.\n1\n[50] Jungseok Hong, Michael Fulton, and Junaed Sattar. TrashCan: A\nsemantically-segmented dataset towards visual detection of marine\ndebris. arXiv:2007.08097, 2020. 8, 17, 18\n[51] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Wein-\nberger. Deep networks with stochastic depth. ECCV, 2016. 15', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='berger. Deep networks with stochastic depth. ECCV, 2016. 15\n[52] Jitesh Jain, Jiachen Li, MangTik Chiu, Ali Hassani, Nikita Orlov,\nand Humphrey Shi. Oneformer: One transformer to rule universal\nimage segmentation. arXiv:2211.06220, 2022. 4\n[53] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,\nHieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig.\nScaling up visual and vision-language representation learning with\nnoisy text supervision. ICML, 2021. 1', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='Scaling up visual and vision-language representation learning with\nnoisy text supervision. ICML, 2021. 1\n[54] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown,\nBenjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey\nWu, and Dario Amodei. Scaling laws for neural language models.\narXiv:2001.08361, 2020. 1\n[55] Michael Kass, Andrew Witkin, and Demetri Terzopoulos. Snakes:\nActive contour models. IJCV, 1988. 4\n[56] Dahun Kim, Tsung-Yi Lin, Anelia Angelova, In So Kweon, and\nWeicheng Kuo.', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='[56] Dahun Kim, Tsung-Yi Lin, Anelia Angelova, In So Kweon, and\nWeicheng Kuo.\nLearning open-world object proposals without\nlearning to classify. IEEE Robotics and Automation Letters, 2022.\n19\n[57] Alexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother,\nand Piotr Doll´ar. Panoptic segmentation. CVPR, 2019. 4\n[58] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan\nKrasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo\nMalloci, Alexander Kolesnikov, Tom Duerig, and Vittorio Ferrari.', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='Malloci, Alexander Kolesnikov, Tom Duerig, and Vittorio Ferrari.\nThe open images dataset v4: Uniﬁed image classiﬁcation, object\ndetection, and visual relationship detection at scale. IJCV, 2020. 2,\n6, 7, 16\n[59] Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and\nThomas Dandres. Quantifying the carbon emissions of machine\nlearning. arXiv:1910.09700, 2019. 28\n[60] Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He. Explor-\ning plain vision transformer backbones for object detection. ECCV,', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='ing plain vision transformer backbones for object detection. ECCV,\n2022. 5, 13, 19, 21, 22, 23\n[61] Yin Li, Zhefan Ye, and James M. Rehg. Delving into egocentric\nactions. CVPR, 2015. 8, 18\n[62] Zhuwen Li, Qifeng Chen, and Vladlen Koltun. Interactive image\nsegmentation with latent diversity. CVPR, 2018. 5, 14, 17\n[63] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr\nDoll´ar. Focal loss for dense object detection. ICCV, 2017. 5, 14', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='Doll´ar. Focal loss for dense object detection. ICCV, 2017. 5, 14\n[64] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro\nPerona, Deva Ramanan, Piotr Doll´ar, and C Lawrence Zitnick. Mi-\ncrosoft COCO: Common objects in context. ECCV, 2014. 2, 4, 6,\n7, 16, 17, 18, 21\n[65] Qin Liu, Zhenlin Xu, Gedas Bertasius, and Marc Niethammer. Sim-\npleClick: Interactive image segmentation with simple vision trans-\nformers. arXiv:2210.11006, 2022. 7, 8, 9, 17', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='formers. arXiv:2210.11006, 2022. 7, 8, 9, 17\n[66] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regu-\nlarization. ICLR, 2019. 15\n[67] Cathy H Lucas, Daniel OB Jones, Catherine J Hollyhead, Robert H\nCondon, Carlos M Duarte, William M Graham, Kelly L Robinson,\nKylie A Pitt, Mark Schildhauer, and Jim Regetz. Gelatinous zoo-\nplankton biomass in the global oceans: geographic variation and\nenvironmental drivers. Global Ecology and Biogeography, 2014.\n18', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='plankton biomass in the global oceans: geographic variation and\nenvironmental drivers. Global Ecology and Biogeography, 2014.\n18\n[68] Sabarinath Mahadevan, Paul Voigtlaender, and Bastian Leibe. Iter-\natively trained interactive segmentation. BMVC, 2018. 4, 15\n[69] Kevis-Kokitsi Maninis, Sergi Caelles, Jordi Pont-Tuset, and Luc\nVan Gool. Deep extreme cut: From extreme points to object seg-\nmentation. CVPR, 2018. 6\n[70] David Martin, Charless Fowlkes, Doron Tal, and Jitendra Malik.\nA database of human segmented natural images and its applica-', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='[70] David Martin, Charless Fowlkes, Doron Tal, and Jitendra Malik.\nA database of human segmented natural images and its applica-\ntion to evaluating segmentation algorithms and measuring ecologi-\ncal statistics. ICCV, 2001. 19, 28\n[71] Fausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi. V-Net:\nFully convolutional neural networks for volumetric medical image\nsegmentation. 3DV, 2016. 5, 14\n[72] Massimo Minervini, Andreas Fischbach, Hanno Scharr, and\nSotirios A. Tsaftaris. Finely-grained annotated datasets for image-', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='Sotirios A. Tsaftaris. Finely-grained annotated datasets for image-\nbased plant phenotyping. Pattern Recognition Letters, 2016. 8, 18\n[73] Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes,\nLucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Debo-\nrah Raji, and Timnit Gebru. Model cards for model reporting. Pro-\nceedings of the conference on fairness, accountability, and trans-\nparency, 2019. 24, 28\n4025\n[74] Dim P Papadopoulos, Jasper RR Uijlings, Frank Keller, and Vittorio\nFerrari. Extreme clicking for efﬁcient object annotation. ICCV,', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='Ferrari. Extreme clicking for efﬁcient object annotation. ICCV,\n2017. 6\n[75] David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-\nMiquel Munguia, Daniel Rothchild, David So, Maud Texier, and\nJeff Dean.\nCarbon emissions and large neural network training.\narXiv:2104.10350, 2021. 28\n[76] Matthew E Peters, Waleed Ammar, Chandra Bhagavatula, and Rus-\nsell Power. Semi-supervised sequence tagging with bidirectional\nlanguage models. Proceedings of the 55th Annual Meeting of the\nAssociation for Computational Linguistics, 2017. 16', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='language models. Proceedings of the 55th Annual Meeting of the\nAssociation for Computational Linguistics, 2017. 16\n[77] Mengyang Pu, Yaping Huang, Yuming Liu, Qingji Guan, and\nHaibin Ling. EDTER: Edge detection with transformer. CVPR,\n2022. 19\n[78] Mattia Pugliatti and Francesco Topputo.\nDOORS: Dataset fOr\nbOuldeRs Segmentation. Zenodo, 2022. 8, 18\n[79] Jiyang Qi, Yan Gao, Yao Hu, Xinggang Wang, Xiaoyu Liu, Xiang\nBai, Serge Belongie, Alan Yuille, Philip Torr, and Song Bai. Oc-', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='Bai, Serge Belongie, Alan Yuille, Philip Torr, and Song Bai. Oc-\ncluded video instance segmentation: A benchmark. IJCV, 2022. 8,\n18, 22, 23, 24\n[80] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh,\nGabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell,\nPamela Mishkin, Jack Clark, et al.\nLearning transferable visual\nmodels from natural language supervision. ICML, 2021. 1, 2, 4, 5,\n7, 9, 13, 21\n[81] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='7, 9, 13, 21\n[81] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea\nVoss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-\nto-image generation. ICML, 2021. 1, 4, 9\n[82] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster\nR-CNN: Towards real-time object detection with region proposal\nnetworks. NeurIPS, 2015. 6, 19\n[83] Xiaofeng Ren and Jitendra Malik. Learning a classiﬁcation model\nfor segmentation. ICCV, 2003. 4', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='[83] Xiaofeng Ren and Jitendra Malik. Learning a classiﬁcation model\nfor segmentation. ICCV, 2003. 4\n[84] Mike Roberts, Jason Ramapuram, Anurag Ranjan, Atulit Kumar,\nMiguel Angel Bautista, Nathan Paczan, Russ Webb, and Joshua M.\nSusskind. Hypersim: A photorealistic synthetic dataset for holistic\nindoor scene understanding. ICCV, 2021. 8, 17, 18\n[85] Candice Schumann, Susanna Ricco, Utsav Prabhu, Vittorio Ferrari,\nand Caroline Pantofaru. A step toward more inclusive people anno-\ntations for fairness. Proceedings of the 2021 AAAI/ACM Conference', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='and Caroline Pantofaru. A step toward more inclusive people anno-\ntations for fairness. Proceedings of the 2021 AAAI/ACM Conference\non AI, Ethics, and Society, 2021. 16\n[86] Seﬁk Ilkin Serengil and Alper Ozpinar. LightFace: A hybrid deep\nface recognition framework. ASYU, 2020. 26\n[87] Seﬁk Ilkin Serengil and Alper Ozpinar. HyperExtended LightFace:\nA facial attribute analysis framework. ICEET, 2021. 26\n[88] Jamie Shotton, John Winn, Carsten Rother, and Antonio Crimin-\nisi. TextonBoost: Joint appearance, shape and context modeling for', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='[88] Jamie Shotton, John Winn, Carsten Rother, and Antonio Crimin-\nisi. TextonBoost: Joint appearance, shape and context modeling for\nmulit-class object recognition and segmentation. ECCV, 2006. 4\n[89] Corey Snyder and Minh Do. STREETS: A novel camera network\ndataset for trafﬁc ﬂow. NeurIPS, 2019. 8, 18\n[90] Konstantin Soﬁiuk, Ilya A Petrov, and Anton Konushin. Reviving\niterative training with mask guidance for interactive segmentation.\nICIP, 2022. 5, 7, 8, 14, 15, 17, 22, 23, 28\n[91] Nitish', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='[91] Nitish\nSrivastava,\nGeoffrey\nHinton,\nAlex\nKrizhevsky,\nIlya\nSutskever, and Ruslan Salakhutdinov. Dropout: A simple way to\nprevent neural networks from overﬁtting. The Journal of Machine\nLearning Research, 2014. 14\n[92] Chris Stauffer and W Eric L Grimson. Adaptive background mix-\nture models for real-time tracking. CVPR, 1999. 4\n[93] Matthew\nTancik,\nPratul\nSrinivasan,\nBen\nMildenhall,', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='Tancik,\nPratul\nSrinivasan,\nBen\nMildenhall,\nSara\nFridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ra-\nmamoorthi, Jonathan Barron, and Ren Ng. Fourier features let net-\nworks learn high frequency functions in low dimensional domains.\nNeurIPS, 2020. 5, 13\n[94] Yansong Tang, Yi Tian, Jiwen Lu, Jianjiang Feng, and Jie Zhou.\nAction recognition in RGB-D egocentric videos. ICIP, 2017. 18\n[95] Yansong Tang, Zian Wang, Jiwen Lu, Jianjiang Feng, and Jie Zhou.', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='[95] Yansong Tang, Zian Wang, Jiwen Lu, Jianjiang Feng, and Jie Zhou.\nMulti-stream deep neural networks for RGB-D egocentric action\nrecognition. IEEE Transactions on Circuits and Systems for Video\nTechnology, 2019. 18\n[96] The\nWorld\nBank.\nThe\nworld\nby\nincome\nand\nregions,\n2022.\nhttps://datatopics.worldbank.org/world-development-\nindicators/the-world-by-income-and-region.html. 16\n[97] Sebastian Thrun. Is learning the n-th thing any easier than learning', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='indicators/the-world-by-income-and-region.html. 16\n[97] Sebastian Thrun. Is learning the n-th thing any easier than learning\nthe ﬁrst? NeurIPS, 1995. 9\n[98] Cameron Trotter, Georgia Atkinson, Matt Sharpe, Kirsten Richard-\nson, A. Stephen McGough, Nick Wright, Ben Burville, and Per\nBerggren.\nNDD20: A large-scale few-shot dolphin dataset for\ncoarse and ﬁne-grained categorisation. arXiv:2005.13359, 2020.\n8, 17, 18, 22, 23, 24\n[99] United States Environmental Protection Agency. Greenhouse Gas', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='8, 17, 18, 22, 23, 24\n[99] United States Environmental Protection Agency. Greenhouse Gas\nEquivalencies Calculator. https://www.epa.gov/energy/greenhouse-\ngas-equivalencies-calculator, 2022. 28\n[100] Koen EA van de Sande, Jasper RR Uijlings, Theo Gevers, and\nArnold WM Smeulders. Segmentation as selective search for ob-\nject recognition. ICCV, 2011. 19\n[101] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,\nLlion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin.', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin.\nAttention is all you need. NeurIPS, 2017. 5, 13\n[102] Boying Wang, Libo Zhang, Longyin Wen, Xianglong Liu, and Yan-\njun Wu. Towards real-world prohibited item detection: A large-\nscale x-ray benchmark. CVPR, 2021. 8, 17, 18\n[103] Weiyao Wang, Matt Feiszli, Heng Wang, Jitendra Malik, and\nDu Tran. Open-world instance segmentation: Exploiting pseudo\nground truth from learned pairwise afﬁnity. CVPR, 2022. 19', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='Du Tran. Open-world instance segmentation: Exploiting pseudo\nground truth from learned pairwise afﬁnity. CVPR, 2022. 19\n[104] Chao-Yuan Wu, Justin Johnson, Jitendra Malik, Christoph Feicht-\nenhofer, and Georgia Gkioxari. Multiview compressive coding for\n3D reconstruction. CVPR, 2023. 9\n[105] Jianxiong Xiao, James Hays, Krista Ehinger, Aude Oliva, and An-\ntonio Torralba. SUN database: Large-scale scene recognition from\nabbey to zoo. CVPR, 2010. 18\n[106] Saining Xie and Zhuowen Tu. Holistically-nested edge detection.', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='[106] Saining Xie and Zhuowen Tu. Holistically-nested edge detection.\nICCV, 2015. 19\n[107] Ning Xu, Brian Price, Scott Cohen, Jimei Yang, and Thomas S\nHuang. Deep interactive object selection. CVPR, 2016. 4, 17\n[108] Kaiyu Yang, Klint Qinami, Li Fei-Fei, Jia Deng, and Olga Rus-\nsakovsky. Towards fairer datasets: Filtering and balancing the dis-\ntribution of the people subtree in the imagenet hierarchy. Proceed-\nings of the 2020 conference on fairness, accountability, and trans-\nparency, 2020. 17', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='ings of the 2020 conference on fairness, accountability, and trans-\nparency, 2020. 17\n[109] Lei Yang, Yan Zi Wei, Yisheng HE, Wei Sun, Zhenhang Huang,\nHaibin Huang, and Haoqiang Fan. iShape: A ﬁrst step towards\nirregular shape instance segmentation. arXiv:2109.15068, 2021. 8,\n18, 22, 23, 24\n[110] Senthil Yogamani, Ciar´an Hughes, Jonathan Horgan, Ganesh Sistu,\nPadraig Varley, Derek O’Dea, Michal Uric´ar, Stefan Milz, Mar-', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='Padraig Varley, Derek O’Dea, Michal Uric´ar, Stefan Milz, Mar-\ntin Simon, Karl Amende, et al. WoodScape: A multi-task, multi-\ncamera ﬁsheye dataset for autonomous driving. ICCV, 2019. 8,\n18\n[111] Lingzhi Zhang, Shenghao Zhou, Simon Stent, and Jianbo Shi. Fine-\ngrained egocentric hand-object segmentation: Dataset, model, and\napplications. ECCV, 2022. 8, 17, 18\n[112] Wenwei Zhang, Jiangmiao Pang, Kai Chen, and Chen Change Loy.\nK-Net: Towards uniﬁed image segmentation. NeurIPS, 2021. 4', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='K-Net: Towards uniﬁed image segmentation. NeurIPS, 2021. 4\n[113] Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-\nWei Chang. Men also like shopping: Reducing gender bias ampliﬁ-\ncation using corpus-level constraints. arXiv:1707.09457, 2017. 16\n[114] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and An-\ntonio Torralba.\nPlaces: A 10 million image database for scene\nrecognition. TPAMI, 2017. 18', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"}), Document(page_content='tonio Torralba.\nPlaces: A 10 million image database for scene\nrecognition. TPAMI, 2017. 18\n[115] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler,\nAdela Barriuso, and Antonio Torralba. Semantic understanding of\nscenes through the ADE20K dataset. IJCV, 2019. 2, 7, 8, 18\n4026', metadata={'source': "/home/se1/se2024/tmp/chatchat/tmpseb9kd72/form-data; name=%22file%22; filename=%22SAM.pdf%22; filename*=UTF-8''SAM.pdf"})]

AI进度：

1. 构建了使用RAG技术进行交互式对话，文献研读，综述生成的基本流程，摒弃了之前的AI组件，并选取了效果更好的组件。LLM使用chatglm3-6b, 词向量嵌入模型使用 bge-large-zh-v1.5, 向量数据库选用便捷易用的 faiss。
2. 目前进度：AI后端接口已测试完毕，AI后端与数据后端正在进行接口对接，下周进行前端与数据后端的接口对接
3. 文献研读进度100：用户可选择一篇文章(自行上传/数据库本来的)开启文献研读，将文章分块计算向量保存，每次提问时会选取语义最相近的K块作为模型的辅助输入，并支持保留对话历史
4. 交互式对话检索进度80：我们将8k篇文章构建大知识库，用户在交互式检索对话时，第一种情况触发关键词检索，去论文库中进行语义检索；第二种情况正常对话交互，我们支持（1）保留上下文（2）从大数据库中选取语义最相近的K块作为模型的辅助输入，有助于AI的输出更精确的结果
5. 文献综述生成进度50：思路类似文献研读，构造不同的提示词即可，多篇文章同理
6. 后续安排：优先整体功能实现，后调优。后续不止支持单个pdf的对话，我们支持用户构建自己的知识库，内容可包含任意类型无结构化文件（md,docx,ppt,pdf...），并使用大模型辅助问答。同时后续可考虑训练微调大模型，使其具备更好效果。